import os
import logging

logger = logging.getLogger(__name__)

def create_llm_model(agent_type: str = "default"):
    """
    Create an LLM model instance based on environment configuration.

    Supports:
    - Azure OpenAI (provider="azure")
    - Google Gemini (provider="google" or "gemini")
    - Ollama (provider="ollama")

    Args:
        agent_type: The type of agent (guide, tourist, scheduler) to look for specific env vars.
                    e.g. GUIDE_MODEL, TOURIST_MODEL, SCHEDULER_MODEL
    """
    from google.adk.models.lite_llm import LiteLlm

    # Determine provider
    provider = os.getenv("MODEL_PROVIDER", "azure").lower()

    # Log proxy configuration
    http_proxy = os.getenv("HTTP_PROXY")
    https_proxy = os.getenv("HTTPS_PROXY")
    logger.info(f"Proxy configuration - HTTP_PROXY: {http_proxy}, HTTPS_PROXY: {https_proxy}")

    # Determine model name
    # 1. Try specific agent model var (e.g. GUIDE_MODEL)
    # 2. Try generic MODEL_NAME
    # 3. Fallback based on provider
    env_var_prefix = agent_type.upper()
    model_name = os.getenv(f"{env_var_prefix}_MODEL")
    if not model_name:
        model_name = os.getenv("MODEL_NAME")

    if provider in ["google", "gemini"]:
        if not model_name:
            model_name = "gemini/gemini-3-pro-preview"

        api_key = os.getenv("GOOGLE_GEMINI_API_KEY")
        if not api_key:
            logger.warning("GOOGLE_GEMINI_API_KEY not set for Gemini model")

        logger.info(f"Creating Gemini model: {model_name}")
        return LiteLlm(
            model=model_name,
            api_key=api_key
        )

    elif provider == "ollama":
        # Ollama provider - local models via Ollama
        if not model_name:
            model_name = os.getenv("OLLAMA_MODEL", "qwen3:latest")

        # LiteLLM expects ollama_chat/ prefix for proper tool calling support
        # Note: Using ollama/ instead of ollama_chat/ causes tool calling issues!
        if not model_name.startswith("ollama_chat/") and not model_name.startswith("ollama/"):
            model_name = f"ollama_chat/{model_name}"
        elif model_name.startswith("ollama/"):
            # Fix: change ollama/ to ollama_chat/ for tool support
            model_name = model_name.replace("ollama/", "ollama_chat/", 1)

        api_base = os.getenv("OLLAMA_HOST", "http://host.docker.internal:11434")

        logger.info(f"Creating Ollama model: {model_name} at {api_base}")
        return LiteLlm(
            model=model_name,
            api_base=api_base,
        )

    elif provider in ["azure", "openai"]:
        deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
        if not model_name:
            model_name = f"azure/{deployment_name}"

        api_key = os.getenv("AZURE_OPENAI_API_KEY")

        api_base = os.getenv("AZURE_OPENAI_ENDPOINT") or os.getenv("AZURE_API_BASE")
        api_version = os.getenv("AZURE_OPENAI_API_VERSION") or os.getenv("AZURE_API_VERSION", "2024-02-01")

        logger.info(f"Creating Azure OpenAI model: {model_name}")
        return LiteLlm(
            model=model_name,
            api_key=api_key,
            api_base=api_base,
            api_version=api_version,
        )

    else:
        # Generic fallback for other providers supported by LiteLLM
        if not model_name:
            model_name = "gpt-3.5-turbo" # Fallback

        logger.info(f"Creating generic LiteLLM model: {model_name}")
        return LiteLlm(model=model_name)
